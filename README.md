# GAttenRNN: A Novel Recurrent Neural Network with Gated Transformer for Enhanced spati-temporal Prediction
## Introduction
![architecture](GAttenRNN_architecture.png)
Spati-temporal prediction, pivotal in diverse applications such as autonomous driving and traffic flow forecasting, faces challenges in capturing complex spati-temporal dependencies. Traditional CNN-based models are limited by narrow receptive fields, hindering their efficiency. To address this, we introduce GAttenRNN, a recurrent neural network incorporating a gated Transformer module and a simplified LSTM network. This design replaces convolutional structures with self-attention augmented by gating mechanisms, enabling global feature extraction and bidirectional memory flow across hierarchical layers. Experimental results on Moving MNIST, TaxiBJ, and KTH datasets demonstrate GAttenRNN's superior performance, achieving significant reductions in Mean Squared Error (MSE) compared to state-of-the-art models. This work highlights the potential of integrating Vision Transformers into RNN frameworks for effective spati-temporal learning without relying on complex model architectures.
